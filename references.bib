@book{Goodfellow-et-al-2016,
	title={Deep Learning},
	author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	publisher={MIT Press},
	note={\url{http://www.deeplearningbook.org}},
	year={2016}
}

@article{Gu_2022,
	title={Thermodynamics of the Ising Model Encoded in Restricted Boltzmann Machines},
	volume={24},
	ISSN={1099-4300},
	url={http://dx.doi.org/10.3390/e24121701},
	DOI={10.3390/e24121701},
	number={12},
	journal={Entropy},
	publisher={MDPI AG},
	author={Gu, Jing and Zhang, Kai},
	year={2022},
	month=nov, pages={1701} }

@article{brush,
	title = {History of the Lenz-Ising Model},
	author = {Brush, Stephen G.},
	journal = {Rev. Mod. Phys.},
	volume = {39},
	issue = {4},
	pages = {883--893},
	numpages = {0},
	year = {1967},
	month = {Oct},
	publisher = {American Physical Society},
	doi = {10.1103/RevModPhys.39.883},
	url = {https://link.aps.org/doi/10.1103/RevModPhys.39.883}
}

@article{onsanger,
	title = {Crystal Statistics. I. A Two-Dimensional Model with an Order-Disorder Transition},
	author = {Onsager, Lars},
	journal = {Phys. Rev.},
	volume = {65},
	issue = {3-4},
	pages = {117--149},
	numpages = {0},
	year = {1944},
	month = {Feb},
	publisher = {American Physical Society},
	doi = {10.1103/PhysRev.65.117},
	url = {https://link.aps.org/doi/10.1103/PhysRev.65.117}
}

@article{kramers,
	title = {Statistics of the Two-Dimensional Ferromagnet. Part I},
	author = {Kramers, H. A. and Wannier, G. H.},
	journal = {Phys. Rev.},
	volume = {60},
	issue = {3},
	pages = {252--262},
	numpages = {0},
	year = {1941},
	month = {Aug},
	publisher = {American Physical Society},
	doi = {10.1103/PhysRev.60.252},
	url = {https://link.aps.org/doi/10.1103/PhysRev.60.252}
}

@article{yang,
	title = {The Spontaneous Magnetization of a Two-Dimensional Ising Model},
	author = {Yang, C. N.},
	journal = {Phys. Rev.},
	volume = {85},
	issue = {5},
	pages = {808--816},
	numpages = {0},
	year = {1952},
	month = {Mar},
	publisher = {American Physical Society},
	doi = {10.1103/PhysRev.85.808},
	url = {https://link.aps.org/doi/10.1103/PhysRev.85.808}
}

@article{Fisher_1967,
	doi = {10.1088/0034-4885/30/2/306},
	url = {https://dx.doi.org/10.1088/0034-4885/30/2/306},
	year = {1967},
	month = {jul},
	publisher = {},
	volume = {30},
	number = {2},
	pages = {615},
	author = {M E Fisher},
	title = {The theory of equilibrium critical phenomena},
	journal = {Reports on Progress in Physics},
	abstract = {The theory of critical phenomena in systems at equilibrium is reviewed at an introductory level with special emphasis on the values of the critical point exponents α, β, γ,..., and their interrelations. The experimental observations are surveyed and the analogies between different physical systems - fluids, magnets, superfluids, binary alloys, etc. - are developed phenomenologically. An exact theoretical basis for the analogies follows from the equivalence between classical and quantal `lattice gases' and the Ising and Heisenberg-Ising magnetic models. General rigorous inequalities for critical exponents at and below Tc are derived. The nature and validity of the `classical' (phenomenological and mean field) theories are discussed, their predictions being contrasted with the exact results for plane Ising models, which are summarized concisely. Padé approximant and ratio techniques applied to appropriate series expansions lead to precise critical-point estimates for the three-dimensional Heisenberg and Ising models (tables of data are presented). With this background a critique is presented of recent theoretical ideas: namely, the `droplet' picture of the critical point and the `homogeneity' and `scaling' hypotheses. These lead to a `law of corresponding states' near a critical point and to relations between the various exponents which suggest that perhaps only two or three exponents might be algebraically independent for any system.}
}

@inbook{Cardy_1996, place={Cambridge}, series={Cambridge Lecture Notes in Physics}, title={Phase transitions in simple Systems}, booktitle={Scaling and Renormalization in Statistical Physics}, publisher={Cambridge University Press}, author={Cardy, John}, year={1996}, pages={1–15}, collection={Cambridge Lecture Notes in Physics}}

@inproceedings{Hinton2012APG,
	title={A Practical Guide to Training Restricted Boltzmann Machines},
	author={Geoffrey E. Hinton},
	booktitle={Neural Networks},
	year={2012},
	url={https://api.semanticscholar.org/CorpusID:21145246}
}

@misc{mehta,
	title={An exact mapping between the Variational Renormalization Group and Deep Learning}, 
	author={Pankaj Mehta and David J. Schwab},
	year={2014},
	eprint={1410.3831},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@misc{morningstar2017deep,
	title={Deep Learning the Ising Model Near Criticality}, 
	author={Alan Morningstar and Roger G. Melko},
	year={2017},
	eprint={1708.04622},
	archivePrefix={arXiv},
	primaryClass={cond-mat.dis-nn}
}


@article{hinton_training_2002,
	title = {Training {Products} of {Experts} by {Minimizing} {Contrastive} {Divergence}},
	volume = {14},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/089976602760128018},
	doi = {10.1162/089976602760128018},
	abstract = {It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual “expert” models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called “contrastive divergence” whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.},
	number = {8},
	journal = {Neural Computation},
	author = {Hinton, Geoffrey E.},
	month = aug,
	year = {2002},
	note = {\_eprint: https://direct.mit.edu/neco/article-pdf/14/8/1771/815447/089976602760128018.pdf},
	pages = {1771--1800},
}

@Article{Oh,
	AUTHOR = {Oh, Sangchul and Baggag, Abdelkader and Nha, Hyunchul},
	TITLE = {Entropy, Free Energy, and Work of Restricted Boltzmann Machines},
	JOURNAL = {Entropy},
	VOLUME = {22},
	YEAR = {2020},
	NUMBER = {5},
	ARTICLE-NUMBER = {538},
	URL = {https://www.mdpi.com/1099-4300/22/5/538},
	PubMedID = {33286309},
	ISSN = {1099-4300},
	ABSTRACT = {A restricted Boltzmann machine is a generative probabilistic graphic network. A probability of finding the network in a certain configuration is given by the Boltzmann distribution. Given training data, its learning is done by optimizing the parameters of the energy function of the network. In this paper, we analyze the training process of the restricted Boltzmann machine in the context of statistical physics. As an illustration, for small size bar-and-stripe patterns, we calculate thermodynamic quantities such as entropy, free energy, and internal energy as a function of the training epoch. We demonstrate the growth of the correlation between the visible and hidden layers via the subadditivity of entropies as the training proceeds. Using the Monte-Carlo simulation of trajectories of the visible and hidden vectors in the configuration space, we also calculate the distribution of the work done on the restricted Boltzmann machine by switching the parameters of the energy function. We discuss the Jarzynski equality which connects the path average of the exponential function of the work and the difference in free energies before and after training.},
	DOI = {10.3390/e22050538}
}

@misc{peyré2020computational,
	title={Computational Optimal Transport}, 
	author={Gabriel Peyré and Marco Cuturi},
	year={2020},
	eprint={1803.00567},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@Inbook{Santambrogio2015,
	author="Santambrogio, Filippo",
	title="Primal and dual problems",
	bookTitle="Optimal Transport for Applied Mathematicians: Calculus of Variations, PDEs, and Modeling",
	year="2015",
	publisher="Springer International Publishing",
	address="Cham",
	pages="1--57",
	abstract="In this chapter we present generalities on the transport problem from a measure $\mu$ on a space X to another measure $\nu$ on a space Y. In general, X and Y can be complete and separable metric spaces, but soon we will focus on the case where they are the same subset $\Omega$⊂ℝd{\$}{\$}{\backslash}varOmega {\backslash}subset {\backslash}mathbb{\{}R{\}}^{\{}d{\}}{\$}{\$}(often compact). The cost function c:X{\texttimes}Y{\textrightarrow}[0,+∞]{\$}{\$}c: X {\backslash}times Y {\backslash}rightarrow [0,+{\backslash}infty ]{\$}{\$}will be possibly supposed to be continuous or semi-continuous, and then we will analyze particular cases (such as c(x,{\thinspace}y){\thinspace}={\thinspace}h(x − y) for strictly convex h). It includes a proof of the existence of an optimal transport plan, a proof of Kantorovich duality, and the Brenier Theorem stating that an optimal transport map for the quadratic cost exists, and is the gradient of a convex function. The discussion section presents connections with problems in economics and finance, and complementary topics such as the multi-marginal case, and a brief survey of the regularity of optimal maps.",
	isbn="978-3-319-20828-2",
	doi="10.1007/978-3-319-20828-2_1",
	url="https://doi.org/10.1007/978-3-319-20828-2_1"
}

@ARTICLE{Rubner2000-ym,
	title    = "The Earth Mover's Distance as a Metric for Image Retrieval",
	author   = "Rubner, Yossi and Tomasi, Carlo and Guibas, Leonidas J",
	abstract = "We investigate the properties of a metric between two
	distributions, the Earth Mover's Distance (EMD), for
	content-based image retrieval. The EMD is based on the minimal
	cost that must be paid to transform one distribution into the
	other, in a precise sense, and was first proposed for certain
	vision problems by Peleg, Werman, and Rom. For image retrieval,
	we combine this idea with a representation scheme for
	distributions that is based on vector quantization. This
	combination leads to an image comparison framework that often
	accounts for perceptual similarity better than other previously
	proposed methods. The EMD is based on a solution to the
	transportation problem from linear optimization, for which
	efficient algorithms are available, and also allows naturally for
	partial matching. It is more robust than histogram matching
	techniques, in that it can operate on variable-length
	representations of the distributions that avoid quantization and
	other binning problems typical of histograms. When used to
	compare distributions with the same overall mass, the EMD is a
	true metric. In this paper we focus on applications to color and
	texture, and we compare the retrieval performance of the EMD with
	that of other distances.",
	journal  = "International Journal of Computer Vision",
	volume   =  40,
	number   =  2,
	pages    = "99--121",
	month    =  nov,
	year     =  2000
}

@Inbook{Ambrosio2005,
	author = "Ambrosio, Luigi and Gigli, Nicola and Savaré, Giuseppe ",
	title="The Wasserstein Distance and its Behaviour along Geodesics",
	bookTitle="Gradient Flows: in Metric Spaces and in the Space of Probability Measures",
	year="2005",
	publisher="Birkh{\"a}user Basel",
	address="Basel",
	pages="151--165",
	isbn="978-3-7643-7309-2",
	doi="10.1007/3-7643-7309-1_9",
	url="https://doi.org/10.1007/3-7643-7309-1_9"
}

@ARTICLE{Kolouri,
	author={Kolouri, Soheil and Park, Se Rim and Thorpe, Matthew and Slepcev, Dejan and Rohde, Gustavo K.},
	journal={IEEE Signal Processing Magazine}, 
	title={Optimal Mass Transport: Signal processing and machine-learning applications}, 
	year={2017},
	volume={34},
	number={4},
	pages={43-59},
	keywords={Linear programming;Data models;Estimation;Probability density function;Transportation;Analytical models;Morphology},
	doi={10.1109/MSP.2017.2695801}}

@incollection{Newman,
	author = {Newman, M E J and Barkema, G T},
	isbn = {9780198517962},
	title = "{The Ising model and the Metropolis algorithm}",
	booktitle = "{Monte Carlo Methods in Statistical Physics}",
	publisher = {Oxford University Press},
	year = {1999},
	month = {02},
	abstract = "{In Section 1.2.2 we introduced the using model, which is one of the simplest and best studied of statistical mechanical models. In this chapter and the next we look in detail at the Monte Carlo methods that have been used to investigate the properties of this model. As well as demonstrating the application of the basic principles described in the last chapter, the study of the using model provides an excellent introduction to the most important Monte Carlo algorithms in use today. Along the way we will also look at some of the tricks used for implementing Monte Carlo algorithms in computer programs and at some of the standard techniques used to analyse the data those programs generate.}",
	doi = {10.1093/oso/9780198517962.003.0003},
	url = {https://doi.org/10.1093/oso/9780198517962.003.0003},
	eprint = {https://academic.oup.com/book/0/chapter/422704903/chapter-pdf/52593551/isbn-9780198517962-book-part-3.pdf},
}

@article{wolff,
	title = {Collective Monte Carlo Updating for Spin Systems},
	author = {Wolff, Ulli},
	journal = {Phys. Rev. Lett.},
	volume = {62},
	issue = {4},
	pages = {361--364},
	numpages = {0},
	year = {1989},
	month = {Jan},
	publisher = {American Physical Society},
	doi = {10.1103/PhysRevLett.62.361},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.62.361}
}

@misc{kingma2017adam,
	title={Adam: A Method for Stochastic Optimization}, 
	author={Diederik P. Kingma and Jimmy Ba},
	year={2017},
	eprint={1412.6980},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@book{krauth06,
	added-at = {2015-05-27T08:13:09.000+0200},
	address = {Oxford},
	author = {Krauth, Werner},
	biburl = {https://www.bibsonomy.org/bibtex/29a1a43c2c5cfc623b99326b5cdd3f777/ytyoun},
	interhash = {28dba7bb421ce8a4ed72d0479e18ca0e},
	intrahash = {9a1a43c2c5cfc623b99326b5cdd3f777},
	isbn = {9781429459501 1429459506},
	keywords = {physics textbook},
	publisher = {Oxford University Press},
	refid = {122336173},
	timestamp = {2015-12-13T12:53:10.000+0100},
	title = {Statistical Mechanics Algorithms and Computations},
	year = 2006
}

@article{torlai_melko,
	title = {Learning thermodynamics with Boltzmann machines},
	author = {Torlai, Giacomo and Melko, Roger G.},
	journal = {Phys. Rev. B},
	volume = {94},
	issue = {16},
	pages = {165134},
	numpages = {7},
	year = {2016},
	month = {Oct},
	publisher = {American Physical Society},
	doi = {10.1103/PhysRevB.94.165134},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.94.165134}
}

@article{bengio_bergstra,
	author = {Bergstra, James and Bengio, Yoshua},
	title = {Random search for hyper-parameter optimization},
	year = {2012},
	issue_date = {3/1/2012},
	publisher = {JMLR.org},
	volume = {13},
	number = {null},
	issn = {1532-4435},
	abstract = {ifferent hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success--they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
	journal = {J. Mach. Learn. Res.},
	month = {feb},
	pages = {281–305},
	numpages = {25},
	keywords = {deep learning, global optimization, model selection, neural networks, response surface modeling}
}

@article{Mehta_2019,
	title={A high-bias, low-variance introduction to Machine Learning for physicists},
	volume={810},
	ISSN={0370-1573},
	url={http://dx.doi.org/10.1016/j.physrep.2019.03.001},
	DOI={10.1016/j.physrep.2019.03.001},
	journal={Physics Reports},
	publisher={Elsevier BV},
	author={Mehta, Pankaj and Bukov, Marin and Wang, Ching-Hao and Day, Alexandre G.R. and Richardson, Clint and Fisher, Charles K. and Schwab, David J.},
	year={2019},
	month=may, pages={1–124} }

@article{Carleo_2019,
	title={Machine learning and the physical sciences},
	volume={91},
	ISSN={1539-0756},
	url={http://dx.doi.org/10.1103/RevModPhys.91.045002},
	DOI={10.1103/revmodphys.91.045002},
	number={4},
	journal={Reviews of Modern Physics},
	publisher={American Physical Society (APS)},
	author={Carleo, Giuseppe and Cirac, Ignacio and Cranmer, Kyle and Daudet, Laurent and Schuld, Maria and Tishby, Naftali and Vogt-Maranto, Leslie and Zdeborová, Lenka},
	year={2019},
	month=dec }

